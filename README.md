# VLM-Instruct-FastGS: Semantic Guidance for Complete Scene Reconstruction

## ğŸ“Œ Overview
VLM-Instruct-FastGS (Vision-Language Model Guided 3D Gaussian Splatting) enhances 3D Gaussian Splatting by leveraging Vision-Language Models (VLMs) to intelligently guide the densification process. Under the same iteration budget, our method achieves more complete scene reconstruction through a three-phase semantic guidance strategy:

- **Phase 1: Accelerated Detail Formation** â€“ Identifies regions that are beginning to show texture detail, accelerating the reconstruction of main subjects during early training.
- **Phase 2: Background Completion** â€“ Detects main subject regions using VLM understanding, then inverts these masks to obtain background areas requiring enhancement, ensuring full scene coverage.
- **Phase 3: Novel View Refinement** â€“ Analyzes renders from unseen viewpoints to identify inconsistent or under-reconstructed regions, further improving rendering quality across the entire scene.

This semantic-aware approach enables comprehensive scene reconstruction without requiring additional iterations or manual annotation.

## ğŸ“Š Performance Comparison
Starting from only 100 random points and after 20,000 iterations, our method, powered by the [Qwen3-VL-2B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct) vision-language model, demonstrates significantly more complete scene reconstruction:

<div align="center"> <table> <tr> <td width="20%"><strong>FastGS</strong></td> <td width="26.7%"><img src="assets/before_1.png" width="100%"></td> <td width="26.7%"><img src="assets/before_2.png" width="100%"></td> <td width="26.7%"><img src="assets/before_3.png" width="100%"></td> </tr> <tr> <td><strong>VLM-Instruct-FastGS (Ours)</strong></td> <td><img src="assets/after_1.png" width="100%"></td> <td><img src="assets/after_2.png" width="100%"></td> <td><img src="assets/after_3.png" width="100%"></td> </tr> </table> </div>
With the same sparse initialization and iteration budget, VLM-Instruct-FastGS builds substantially more scene structureâ€”particularly in background regions. The semantic guidance enables the model to allocate Gaussians more intelligently, resulting in more comprehensive scene coverage from the very early stages of training.

## ğŸ“Š Result
We evaluate our method on the Mip-NeRF 360 dataset, comparing training loss convergence against vanilla FastGS under the same sparse initialization (100 random points)

<div align="center"> <img src="assets/Figure_1.png" width="80%"> <br> <em>Training loss comparison on Mip-NeRF 360 dataset</em> </div>

Phase 1 (0â€“8,000 iterations): Rapidly reconstructs main scene subjects.

Phase 2 (8,000â€“14,000 iterations): Background Completion â€“ Identifies and inverts main subject masks to target background areas, perfecting comprehensive scene coverage beyond foreground objects.

Pruning & Refinement (14,000â€“20,000 iterations):  Prunes redundant Gaussians to reduce computational burden.

## ğŸ› ï¸ Preparation
### Download VLM Model
Download the [Qwen3-VL-2B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct) vision-language model and place it in the appropriate directory.
### Dataset Structure
Organize your single scene dataset as follows:
```bash
â”œâ”€â”€ your_project/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ sparse/
â”‚       â””â”€â”€ 0/
â”‚           â”œâ”€â”€ cameras.bin
â”‚           â””â”€â”€ images.bin
```
